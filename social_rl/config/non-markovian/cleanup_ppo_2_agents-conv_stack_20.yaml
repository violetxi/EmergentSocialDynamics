Environment:
  base_env_kwargs:
    env: 'cleanup'
    num_agents: 2
    use_collective_reward: False
    inequity_averse_reward: False
    alpha: 0.0
    beta: 0.0
  max_cycles: 2000    # this decides length of an episode
  render_mode: rgb_array
  stack_num: 20
  
Net:
  in_channels: 20
  out_channels: 6
  kernel_size: 3
  stride: 1
  padding: 0
  bias: True
  flatten_dim: 1014
  num_layers: 2
  hidden_dim: 64
  output_dim: 64

PPOPolicy:
  max_grad_norm: 0.5
  eps_clip: 0.2
  vf_coef: 0.5
  ent_coef: 0.1    # key hyperparameter to tune
  gae_lambda: 0.95
  reward_normalization: 0
  dual_clip: null
  value_clip: 0
  deterministic_eval: True
  advantage_normalization: 0
  recompute_advantage: 0

