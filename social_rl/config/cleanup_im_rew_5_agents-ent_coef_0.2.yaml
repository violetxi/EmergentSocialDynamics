Environment:
  base_env_kwargs:
    env: 'cleanup'
    num_agents: 5
    use_collective_reward: False
    inequity_averse_reward: False
    alpha: 0.0
    beta: 0.0
  max_cycles: 2000    # this decides length of an episode
  render_mode: rgb_array
  
Net:
  in_channels: 1
  out_channels: 6
  kernel_size: 3
  stride: 1
  padding: 0
  bias: True
  flatten_dim: 1014
  num_layers: 2
  hidden_dim: 64
  output_dim: 64

PPOPolicy:
  max_grad_norm: 0.5
  eps_clip: 0.2
  vf_coef: 0.5
  ent_coef: 0.2    # key hyperparameter to tune
  gae_lambda: 0.95
  reward_normalization: 0
  dual_clip: null
  value_clip: 0
  deterministic_eval: True
  advantage_normalization: 0
  recompute_advantage: 0

IMPolicy:  
  module_path: 'social_rl.policy.ic_reward_policy'
  class_name: 'IMRewardPolicy'
  # world model configuration
  world_model:
    # path to the model class
    module_path: 'social_rl.model.ic_reward_pred_net'
    class_name: 'IMRewardModule'
    # path to the arguments of the model class
    args:
      # encoder network to encode obs to latent state
      feature_net:
        module_path: 'social_rl.model.core'
        class_name: 'CNNICM'
        config:
          in_channels: 1
          out_channels: 6
          kernel_size: 3
          stride: 1
          padding: 0
          bias: True
          flatten_dim: 1014
          num_layers: 2
          hidden_dim: 64
          output_dim: 64
    # required args for world model
    kwargs:
      feature_dim: 64
      action_dim: 9
      num_other_agents: 4
      hidden_sizes: [64, 64]
  # world model based policy args
  args:
    lr_scale: 1.0
    reward_scale: 1.0
    forward_loss_weight: 1.0
